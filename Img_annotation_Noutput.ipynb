{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 图片预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# '''----------------test-------------------'''\n",
    "# test = cv2.imread( 'Flickr8k_Dataset\\\\Flicker8k_Dataset\\\\'+img_names[0])\n",
    "# print('h,w,c = ',test.shape)\n",
    "# test = cv2.resize(test,(224,224))\n",
    "# print('h,w,c = ',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def reshape(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img,(224,224))  \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.listdir(filename)返回filename中所有文件的文件名列表\n",
    "img_names = os.listdir('Flickr8k_Dataset\\Flicker8k_Dataset')\n",
    "img_num = len(img_names)\n",
    "img_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''----------------test-------------------'''\n",
    "# x = np.expand_dims(test, axis=0)  # 展开\n",
    "# x = preprocess_input(x)       # 预处理到0～1\n",
    "# out = vgg19.predict(x)\n",
    "\n",
    "# print(out.shape)\n",
    "# type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mapping = dict()  \n",
    "for i in range(img_num):\n",
    "    img_path =  'Flickr8k_Dataset\\\\Flicker8k_Dataset\\\\'+img_names[i]\n",
    "    img = reshape(img_path)\n",
    "    name = img_names[i]\n",
    "    img_mapping[name] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''===============test==============='''\n",
    "img_mapping[img_names[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if not os.path.exists(os.path.abspath('.') + '\\\\img_mapping.npy'):    \n",
    "    with open('img_mapping.pkl', 'wb') as df1:\n",
    "        pickle.dump(img_mapping, df1, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('img_mapping.pkl', 'rb') as df2:\n",
    "        img_mapping = pickle.load(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 处理语句信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# load .txt file as text\n",
    "def load_doc(txt_filepath):\n",
    "    # open the file as read only\n",
    "    file = open(txt_filepath, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract descriptions for images\n",
    "def load_text(text):\n",
    "    imgs=[]\n",
    "    words = []\n",
    "    img_ids = []\n",
    "    annots = []\n",
    "    for line in text.split('\\n'):\n",
    "        \n",
    "        tokens = line.split()\n",
    "        if len(line) > 1:\n",
    "            \n",
    "            # take the first token as the image id, the rest as the description\n",
    "            img_id, img_wd = tokens[0], tokens[1:]\n",
    "            # remove filename from image id\n",
    "            img_id = img_id.split('#')[0]\n",
    "            \n",
    "            #build the list of ids\n",
    "            if img_id not in img_ids:\n",
    "                img_ids.append(img_id)\n",
    "                imgs.append(img_mapping[img_id])\n",
    "                annots.append([])\n",
    "            \n",
    "            # build a set of words\n",
    "            for word in img_wd:\n",
    "                word.strip('.,\\\"')\n",
    "                word.lower()\n",
    "                if word not in words:\n",
    "                    words.append(word)\n",
    "                    \n",
    "            # convert description tokens back to string        \n",
    "            annot = ' '.join(img_wd)\n",
    "            #build the list of ids\n",
    "            annots[-1].append(annot)\n",
    "            \n",
    "    return img_ids,imgs, annots, words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = 'Flickr8k_text\\Flickr8k.lemma.token.txt'\n",
    "text = load_doc(text_path)\n",
    "ids, imgs, annots, words =load_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "#import itertools \n",
    "z = list(zip( imgs, annots))\n",
    "random.shuffle(z)\n",
    "imgs, annots = zip(*z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_len = len(words)\n",
    "wd_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#annots = np.array(annots)\n",
    "# annots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [item for sublist in annots for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.preprocessing.text as T\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts([item for sublist in annots for item in sublist])\n",
    "# print(tokenizer.word_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots_=annots\n",
    "annots_\n",
    "k = 0\n",
    "while k < 8091:\n",
    "    p = 0\n",
    "    while p < 5:\n",
    "        a = annots_[k][p]\n",
    "        a = T.text_to_word_sequence(a)\n",
    "        a = tokenizer.texts_to_sequences(a)\n",
    "        a = [item for sublist in a for item in sublist]\n",
    "        #a = np.array(a)\n",
    "        print(a)\n",
    "        annots_[k][p]=a\n",
    "        p += 1\n",
    "    k += 1\n",
    "#annots_ = np.array(annots_)\n",
    "# annots_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annots_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''----------------test-------------------'''\n",
    "# import keras.preprocessing.text as T\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer(num_words=None)\n",
    "# tokenizer.fit_on_texts(texts)\n",
    "# text1='some thing to eat'\n",
    "# text2='some thing to drink'\n",
    "# texts=[text1,text2]\n",
    "\n",
    "# print (T.text_to_word_sequence(text2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 制做数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "img_train, img_test, annot_train, annot_test = train_test_split(imgs,annots_,test_size=0.25, random_state=42)\n",
    "img_test, img_val, annot_test, annot_val = train_test_split(img_test, annot_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "# define the decoder model\n",
    "def define_model(vocab_size, max_length):\n",
    "    \n",
    "    pretrained_cnn = VGG19(weights='imagenet', include_top=False)\n",
    "    pretrained_cnn.layers.pop()\n",
    "    # pretrained_cnn.trainable = True\n",
    "    for layer in pretrained_cnn.layers[:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(224,224,3))\n",
    "#     Encoder = VGG19(weights='imagenet', include_top=True)\n",
    "#     Encoder.layers.pop()\n",
    "    fe0 = pretrained_cnn(inputs1)\n",
    "    fe1 = GlobalAveragePooling2D()(fe0)\n",
    "    fe2 = Dropout(0.5)(fe1)\n",
    "    fe3 = Dense(256, activation='relu')(fe2)\n",
    "\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # decoder model\n",
    "    decoder1 = add([fe3, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = define_model(vocab_size, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annot_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the below function loop forever with a while loop and within this, \n",
    "#loop over each image in the image directory. \n",
    "#For each image filename, we can load the image and \n",
    "#create all of the input-output sequence pairs from the image’s description.\n",
    "\n",
    "#data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(annots_train_r,img_train, max_length):\n",
    "    while 1:\n",
    "        for k in range(6068):\n",
    "            #retrieve photo features\n",
    "            img = img_train[k]\n",
    "            annots_5 = annot_train[k]\n",
    "            input_image, input_sequence, output_word = create_sequences( max_length, annots_5, img)\n",
    "            yield [[input_image, input_sequence], output_word]\n",
    "\n",
    "            \n",
    "#we are calling the create_sequence() function to create \n",
    "#a batch worth of data for a single photo rather than an entire dataset. \n",
    "#This means that we must update the create_sequences() function \n",
    "#to delete the “iterate over all descriptions” for-loop.            \n",
    "#Updated create sequence function for data_generator\n",
    "def create_sequences(max_length, annots_5, img):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for annot in annots_5:\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for s in range(1, len(annot)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = annot[:s], annot[s]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(img)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    \n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_epoch = 0\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#   start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "steps = len(annot_train)\n",
    "max_length = 30\n",
    "# from time import time\n",
    "# random.seed(time)\n",
    "for i in range(epochs):\n",
    "#    annot_train_r = []\n",
    "#     for j in range(6068):\n",
    "#         index = random.randint(0,4)\n",
    "#         annot_train_r.append(annot_train[j][index])\n",
    "    generator = data_generator(annot_train, img_train , max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    #model.fit(x={'inputs1': img_train, 'inputs2': annot_train_r}, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
